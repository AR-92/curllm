#!/usr/bin/env bash

# utils.sh - Utility functions for curllm

# Function to display help
show_help() {
    echo "curllm - A pure Bash LLM API wrapper"
    echo ""
    echo "Usage:"
    echo "  curllm [command] [options]"
    echo ""
    echo "Commands:"
    echo "  chat <prompt>     Send a chat completion request"
    echo "  help              Show this help message"
    echo "  version           Show version information"
    echo ""
    echo "Options:"
    echo "  --provider <name>  Specify provider (openai, anthropic, qwen, mistral, gemini, openrouter, groq)"
    echo "  --model <name>     Specify model name"
    echo "  --verbose          Enable verbose logging"
    echo "  --help, -h         Show help for a command"
    echo "  --version, -v      Show version"
    echo ""
    echo "Supported Providers:"
    echo "  openai      - OpenAI GPT models"
    echo "  anthropic   - Anthropic Claude models"
    echo "  qwen        - Alibaba Qwen models"
    echo "  mistral     - Mistral AI models"
    echo "  gemini      - Google Gemini models"
    echo "  openrouter  - OpenRouter models"
    echo "  groq        - Groq models"
    echo ""
    echo "Configuration:"
    echo "  Config file: ~/.config/curllm/config"
    echo "  Default provider and model can be set in config file"
    echo "  Log file: ~/.cache/curllm/curllm.log"
    echo ""
    echo "Environment Variables:"
    echo "  MOCK_MODE=true     Enable mock mode (no real API calls)"
    echo "  XDG_CONFIG_HOME    Config directory (default: ~/.config)"
    echo "  XDG_CACHE_HOME     Log directory (default: ~/.cache)"
    echo ""
    echo "Examples:"
    echo "  curllm chat \"What is the capital of France?\""
    echo "  curllm chat --provider qwen \"Explain quantum computing\""
    echo "  curllm chat --provider groq --model llama3-8b-8192 \"What is Bash?\""
    echo "  curllm chat --verbose \"Debug this request\""
    echo "  MOCK_MODE=true curllm chat \"Test prompt\""
    echo ""
    echo "For more information, visit: https://github.com/yourusername/curllm"
}
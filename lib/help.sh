#!/usr/bin/env bash

# help.sh - Comprehensive help system for curllm

# Function to display basic help
show_basic_help() {
    echo "curllm - Universal LLM API Wrapper"
    echo "Version: v0.1.0"
    echo ""
    echo "USAGE:"
    echo "  curllm [COMMAND] [OPTIONS] [ARGUMENTS]"
    echo ""
    echo "COMMANDS:"
    echo "  chat <prompt>     Send a chat completion request to an LLM"
    echo "  help             Show this help message"
    echo "  version          Show version information"
    echo ""
    echo "BASIC EXAMPLES:"
    echo "  curllm chat \"What is the capital of France?\""
    echo "  curllm chat --provider qwen \"Explain quantum computing\""
    echo "  curllm chat --provider groq --model llama3-8b-8192 \"Write a Python function\""
    echo ""
    echo "For detailed documentation, visit: https://github.com/AR-92/curllm"
    echo "Use 'curllm help --verbose' for comprehensive help"
}

# Function to display the current help (matching utils.sh)
show_help() {
    echo "curllm - A pure Bash LLM API wrapper"
    echo ""
    echo "Usage:"
    echo "  curllm [command] [options]"
    echo ""
    echo "Commands:"
    echo "  chat <prompt>     Send a chat completion request"
    echo "  help              Show this help message"
    echo "  version           Show version information"
    echo ""
    echo "Options:"
    echo "  --provider <name>  Specify provider (openai, anthropic, qwen, mistral, gemini, openrouter, groq)"
    echo "  --model <name>     Specify model name"
    echo "  --verbose          Enable verbose logging"
    echo "  --help, -h         Show help for a command"
    echo "  --version, -v      Show version"
    echo ""
    echo "Supported Providers:"
    echo "  openai      - OpenAI GPT models"
    echo "  anthropic   - Anthropic Claude models"
    echo "  qwen        - Alibaba Qwen models"
    echo "  mistral     - Mistral AI models"
    echo "  gemini      - Google Gemini models"
    echo "  openrouter  - OpenRouter models"
    echo "  groq        - Groq models"
    echo ""
    echo "Configuration:"
    echo "  Config file: ~/.config/curllm/config"
    echo "  Default provider and model can be set in config file"
    echo "  Log file: ~/.cache/curllm/curllm.log"
    echo ""
    echo "Environment Variables:"
    echo "  MOCK_MODE=true     Enable mock mode (no real API calls)"
    echo "  XDG_CONFIG_HOME    Config directory (default: ~/.config)"
    echo "  XDG_CACHE_HOME     Log directory (default: ~/.cache)"
    echo ""
    echo "Examples:"
    echo "  curllm chat \"What is the capital of France?\""
    echo "  curllm chat --provider qwen \"Explain quantum computing\""
    echo "  curllm chat --provider groq --model llama3-8b-8192 \"What is Bash?\""
    echo "  curllm chat --verbose \"Debug this request\""
    echo "  MOCK_MODE=true curllm chat \"Test prompt\""
    echo ""
    echo "For more information, visit: https://github.com/AR-92/curllm"
}

# Function to display verbose help
show_verbose_help() {
    echo "curllm - Universal LLM API Wrapper"
    echo "================================"
    echo ""
    echo "A powerful, pure Bash LLM API wrapper that provides a unified interface"
    echo "to interact with multiple Large Language Model providers."
    echo ""
    echo "SUPPORTED PROVIDERS:"
    echo "  openai      OpenAI GPT models (gpt-3.5-turbo, gpt-4, etc.)"
    echo "  anthropic   Anthropic Claude models (claude-2, claude-instant, etc.)"
    echo "  qwen        Alibaba Qwen models (qwen-turbo, qwen-plus, etc.)"
    echo "  mistral     Mistral AI models (mistral-tiny, mistral-small, etc.)"
    echo "  gemini      Google Gemini models (gemini-pro, gemini-pro-vision, etc.)"
    echo "  openrouter  OpenRouter models (100+ models from various providers)"
    echo "  groq        Groq models (llama2, llama3, mixtral, etc.)"
    echo ""
    echo "USAGE:"
    echo "  curllm COMMAND [OPTIONS] [ARGUMENTS]"
    echo ""
    echo "COMMANDS:"
    echo "  chat <prompt>"
    echo "      Send a chat completion request to an LLM provider"
    echo "      Arguments:"
    echo "        prompt    The text prompt to send to the LLM"
    echo ""
    echo "  help"
    echo "      Show help information"
    echo "      Options:"
    echo "        --verbose    Show detailed help information"
    echo ""
    echo "  version"
    echo "      Show version information"
    echo ""
    echo "OPTIONS:"
    echo "  --provider <name>"
    echo "      Specify the LLM provider to use"
    echo "      Default: openai"
    echo "      Values: openai, anthropic, qwen, mistral, gemini, openrouter, groq"
    echo ""
    echo "  --model <name>"
    echo "      Specify the model to use"
    echo "      Default: Provider-specific (e.g., gpt-3.5-turbo for OpenAI)"
    echo "      Examples: gpt-4, claude-2, qwen-turbo, llama3-8b-8192"
    echo ""
    echo "  --verbose"
    echo "      Enable verbose logging for debugging"
    echo "      Creates detailed logs in ~/.cache/curllm/curllm.log"
    echo ""
    echo "  --help, -h"
    echo "      Show help information"
    echo ""
    echo "  --version, -v"
    echo "      Show version information"
    echo ""
    echo "CONFIGURATION:"
    echo "  Configuration file: ~/.config/curllm/config"
    echo "  Required: API keys for each provider you want to use"
    echo "  Example config:"
    echo "    DEFAULT_PROVIDER=openai"
    echo "    DEFAULT_MODEL=gpt-3.5-turbo"
    echo "    OPENAI_API_KEY=sk-your-key"
    echo "    ANTHROPIC_API_KEY=your-key"
    echo ""
    echo "ENVIRONMENT VARIABLES:"
    echo "  MOCK_MODE=true        Enable mock mode (no real API calls)"
    echo "  XDG_CONFIG_HOME       Configuration directory (default: ~/.config)"
    echo "  XDG_CACHE_HOME        Log directory (default: ~/.cache)"
    echo ""
    echo "EXAMPLES:"
    echo "  # Basic usage with default provider"
    echo "  curllm chat \"What is the capital of France?\""
    echo ""
    echo "  # Specify provider"
    echo "  curllm chat --provider qwen \"Explain machine learning\""
    echo ""
    echo "  # Specify provider and model"
    echo "  curllm chat --provider groq --model llama3-8b-8192 \"Write a bash script\""
    echo ""
    echo "  # Enable verbose logging"
    echo "  curllm chat --verbose \"Debug this complex prompt\""
    echo ""
    echo "  # Use mock mode for testing"
    echo "  MOCK_MODE=true curllm chat \"Test without API keys\""
    echo ""
    echo "  # Multi-line prompt using heredoc"
    echo "  curllm chat << 'EOF'"
    echo "  Analyze this code and suggest improvements:"
    echo "  def fibonacci(n):"
    echo "      if n <= 1:"
    echo "          return n"
    echo "      else:"
    echo "          return fibonacci(n-1) + fibonacci(n-2)"
    echo "  EOF"
    echo ""
    echo "LOGGING:"
    echo "  Log file: ~/.cache/curllm/curllm.log"
    echo "  Log levels: ERROR, WARN, INFO, DEBUG"
    echo "  Enable DEBUG level with --verbose flag"
    echo ""
    echo "SECURITY:"
    echo "  API keys are stored in ~/.config/curllm/config"
    echo "  Set permissions: chmod 600 ~/.config/curllm/config"
    echo "  Keys are validated before making API requests"
    echo ""
    echo "DEVELOPMENT:"
    echo "  Extensive test suite: 50+ test files"
    echo "  Mock mode for testing without API calls"
    echo "  Modular provider system for easy extensions"
    echo ""
    echo "For complete documentation, visit: https://github.com/AR-92/curllm"
}